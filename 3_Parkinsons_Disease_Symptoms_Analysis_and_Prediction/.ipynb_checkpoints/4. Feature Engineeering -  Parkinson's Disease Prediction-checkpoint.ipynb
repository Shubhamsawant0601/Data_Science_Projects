{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9335f9b4",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdd89b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries for ML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4be967",
   "metadata": {},
   "source": [
    "# Importing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2d099",
   "metadata": {},
   "source": [
    "# 1. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09744d",
   "metadata": {},
   "source": [
    "## Selecting the features based upon mutual information gain in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into dependant and independant variables\n",
    "x = df.drop('status', axis=1)\n",
    "y = df['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22999576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for selecting the features based upon mutual information gain in classification\n",
    "# Values ranges from 0-1 higher is better and that variable selected\n",
    "\n",
    "def select_features_mutual_info_classification(features, target, threashold):\n",
    "    mutual_info = mutual_info_classif(features, target)\n",
    "    mutual_data = pd.Series(mutual_info,index = features.columns)\n",
    "    top_feature = mutual_data.sort_values(ascending=False)\n",
    "    return top_feature[top_feature>threashold] \n",
    "\n",
    "top_features = select_features_mutual_info_classification(x, y, 0.1)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deleted1 = df[list(top_features.index)]\n",
    "df_deleted1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f8188",
   "metadata": {},
   "source": [
    "## Deleting the unnecessary columns with correlation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe133679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations matrix\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(df_deleted1.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "\n",
    "# find and remove correlated features\n",
    "# we will create function which will find the columns having highest correlation with each other\n",
    "# we will delete the columns which gives highest correlation as if two columns having highest correlation above threashold then, \n",
    "# it will be considered as both columns gives same information gain.\n",
    "\n",
    "def columns_to_delete_due_to_high_correlation(dataset, threshold):\n",
    "    col_corr = set()                                        # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:     # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]            # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac504fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns which can be deleted due to high correlation\n",
    "cols_delete = columns_to_delete_due_to_high_correlation(df_deleted1, threshold)\n",
    "list(cols_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the columns\n",
    "df_removed_cols = df.drop(list(cols_delete), axis=1)\n",
    "df_removed_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As 'name' column is not required we will delete the column\n",
    "df_removed_cols = df_removed_cols.drop('name', axis=1)\n",
    "df_removed_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c462c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed_cols.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701db2b",
   "metadata": {},
   "source": [
    "### Conclusion - \n",
    "\n",
    "- We have used the Mutual information gain method for classification to get the features which are most important for classification based on dependant and indepedant variables\n",
    "- Then we deleted the columns columns which are having lower mutual information gain score than 0.1\n",
    "- We have then deleted the columns which are having the highest correlation score, considering the fact that if two columns are having highest correlation score then both columns are giving the same information gain and weightage to the output.\n",
    "\n",
    "- We have selected the below columns for further analysis and modelling\n",
    "\n",
    "- ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)',\n",
    "       'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP', 'MDVP:APQ', 'NHR', 'HNR',\n",
    "       'status', 'RPDE', 'DFA', 'spread2', 'D2', 'PPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023af97",
   "metadata": {},
   "source": [
    "# 2. Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the status column values to see the imbalance in the dataset\n",
    "positive = df_removed_cols[df_removed_cols[\"status\"]==1]\n",
    "negative = df_removed_cols[df_removed_cols[\"status\"]==0]\n",
    "\n",
    "# shape of the data\n",
    "print(positive.shape, negative.shape)\n",
    "\n",
    "# plot the count of records for status 1 and 0\n",
    "df_removed_cols[\"status\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9427868",
   "metadata": {},
   "source": [
    "### From above we can conclude that data is imbalanced.\n",
    "As we can see dataset having 140 records for postive while only 49 records for negative outcomes. Hence we will upsample the dataset for negative outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fa937",
   "metadata": {},
   "source": [
    "### Upsampling -\n",
    "\n",
    "- We don't reduce the dataset from class having maximum instances but we rather increase the instances of class who have less dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_removed_cols.drop([\"status\"], axis=1)\n",
    "y = df_removed_cols[\"status\"]\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "over_sampler = RandomOverSampler()\n",
    "X_res, Y_res = over_sampler.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324369be-ddc5-4eac-9a5a-3b319b9b3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the datasets after upsampling\n",
    "df_upsample = X_res\n",
    "df_upsample['status'] = Y_res\n",
    "df_upsample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb399e4-e636-4be4-aa49-cdfe9753a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the status column values to see the imbalance in the dataset\n",
    "positive = df_upsample[df_upsample[\"status\"]==1]\n",
    "negative = df_upsample[df_upsample[\"status\"]==0]\n",
    "\n",
    "# shape of the data\n",
    "print(positive.shape, negative.shape)\n",
    "\n",
    "# plot the count of records for status 1 and 0\n",
    "df_upsample[\"status\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22312997",
   "metadata": {},
   "source": [
    "# 3. Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ee894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataframe for columns and its unique count of values, datatypes and type of columns\n",
    "def find_categorical_continous_variables(data, threashold):\n",
    "    dic = {}\n",
    "    continus_cols = []\n",
    "    categorical_cols = []\n",
    "\n",
    "    for column in data.columns:\n",
    "        li = []\n",
    "        li.append(len(data[column].unique()))\n",
    "        li.append(data[column].dtype)\n",
    "\n",
    "        # Check if the column is categorical or continuous\n",
    "        if len(data[column].unique()) <= threashold or data[column].dtype == 'O':\n",
    "            li.append(\"Categorical column\")\n",
    "            categorical_cols.append(column)\n",
    "        else:\n",
    "            li.append(\"Continous Column\")\n",
    "            continus_cols.append(column)\n",
    "\n",
    "        dic[column] = li\n",
    "    \n",
    "    # create dataframe for columns and its details\n",
    "    dic_df = pd.DataFrame(dic, index=[\"Unique values\", \"Data Type\", \"Categorical/Continous\"])\n",
    "    \n",
    "    # return the dataframe, and lists for continous and categorical columns\n",
    "    return dic_df.T, categorical_cols, continus_cols\n",
    "\n",
    "# Function to find descriptive statistics by providing it the contious cols and dataframe\n",
    "def descriptive_statistics_continous(df, continus_cols):\n",
    "    dic={}\n",
    "    for col in continus_cols:\n",
    "        dic[col] = []\n",
    "        dic[col].append(df[col].mean())\n",
    "        dic[col].append(df[col].median())\n",
    "        dic[col].append(df[col].mode()[0])\n",
    "        dic[col].append(df[col].std())\n",
    "        dic[col].append(df[col].var())\n",
    "        dic[col].append(df[col].max() - df[col].min())\n",
    "        dic[col].append(df[col].quantile(0.75) - df[col].quantile(0.25) )\n",
    "        dic[col].append(df[col].skew())\n",
    "        dic[col].append(df[col].kurtosis())\n",
    "\n",
    "    df_details = pd.DataFrame(dic, index=[\"Mean\", \"Median\", \"Mode\", \"Std Deviation\", \"Variance\", \"Range\", \"IQR\", \"Skew\", \"Kurtosis\"])\n",
    "    df_details = df_details.T\n",
    "    \n",
    "    # find out the skenewss\n",
    "    df_details.loc[df_details[\"Skew\"]<=-0.5, \"Skeness\"] = \"Left/Negative Skew\"\n",
    "    df_details.loc[df_details[\"Skew\"]>=0.5, \"Skeness\"] = \"Right/Positive Skew\"\n",
    "    df_details.loc[(df_details[\"Skew\"]<0.5) & (df_details[\"Skew\"]>-0.5), \"Skeness\"] = \"Symmetric\"\n",
    "    \n",
    "    # find out the kurtosis\n",
    "    df_details.loc[df_details[\"Kurtosis\"]<2.5, \"Kurtosis_type\"] = \"Platykurtic\"\n",
    "    df_details.loc[df_details[\"Kurtosis\"]>3.5, \"Kurtosis_type\"] = \"Leptokurtic\"\n",
    "    df_details.loc[(df_details[\"Kurtosis\"]<3.5) & (df_details[\"Kurtosis\"]>2.5), \"Kurtosis_type\"] = \"Mesokurtic\"\n",
    "    \n",
    "    return df_details\n",
    "\n",
    "# lets find out the unique counts, datatypes, variable type like continous/categorical and lists for columns names having continous/categorical columns\n",
    "col_type_df, categorical_cols, continus_cols = find_categorical_continous_variables(df_temp, 10)\n",
    "\n",
    "# Get the descriptive statistics\n",
    "df_distribution = descriptive_statistics_continous(df_temp, continus_cols)\n",
    "df_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_numerical(df):\n",
    "\n",
    "    # Lets plot the histogram for each variable\n",
    "    features = df.select_dtypes(include='number').columns\n",
    "\n",
    "    # plot the subplot for histogram of each variable\n",
    "    fig, axs = plt.subplots(len(df.columns), 3, figsize=(20, 60))\n",
    "    row = 0\n",
    "    for feature in features:\n",
    "\n",
    "        axs[row, 0].set_title(\"Histogram for {}\".format(feature))\n",
    "        sns.histplot(data=df, x=feature, kde=True, color=\"red\", ax=axs[row, 0])\n",
    "\n",
    "        axs[row, 1].set_title(\"Boxplot for {}\".format(feature))\n",
    "        sns.boxplot(data=df, x=feature,  color=\"skyblue\", ax=axs[row, 1])\n",
    "\n",
    "        axs[row, 2].set_title(\"Vaiolinplot for {}\".format(feature))\n",
    "        sns.violinplot(data=df, x=feature,  color=\"lightgreen\", ax=axs[row, 2])\n",
    "        row = row+1\n",
    "\n",
    "    plt.title(\"Histogram, Boxplot and Violinplots for all variables\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_distribution_numerical(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7622282",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_skewed_columns = [df_distribution[\"Skeness\"]=='Right/Positive Skew'].index\n",
    "print(right_skewed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_distributed_columns = df_distribution[df_distribution[\"Skeness\"]=='Symmetric'].index\n",
    "print(normal_distributed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bb7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_skewed_columns= df_distribution[df_distribution[\"Skeness\"]=='Left/Negative Skew'].index\n",
    "print(left_skewed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340fdab-e941-485d-8f1a-d4300d96ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find out which feature transformation method for which column\n",
    "def select_feature_transformation_methods(X):\n",
    "\n",
    "    # Initialize dictionary to store selected transformation methods for each column\n",
    "    transformation_methods = {}\n",
    "    df_transformed = pd.DataFrame()\n",
    "\n",
    "    # Loop through each column of the dataset\n",
    "    for col_name in X.columns:\n",
    "        column_data = X[col_name].to_numpy()  # Convert Pandas Series to NumPy array\n",
    "\n",
    "        # Check if column contains only positive values and a wide range\n",
    "        if np.all(column_data > 0) and (np.max(column_data) - np.min(column_data)) > 10:\n",
    "            transformation_methods[col_name] = 'logarithmic'\n",
    "            df_transformed[col_name] = np.log1p(column_data)\n",
    "\n",
    "        # Check if column has large values and heavy tails\n",
    "        elif np.max(column_data) > 100 and np.ptp(column_data) > 100:\n",
    "            transformation_methods[col_name] = 'square'\n",
    "            df_transformed[col_name] = np.square(column_data)\n",
    "\n",
    "        # Check if column has small values and heavy tails\n",
    "        elif np.max(column_data) < 10 and np.ptp(column_data) > 10:\n",
    "            transformation_methods[col_name] = 'reciprocal'\n",
    "            df_transformed[col_name] = np.reciprocal(column_data)\n",
    "\n",
    "        # For other cases, use Box-Cox or Yeo-Johnson transformation\n",
    "        else:\n",
    "            try:\n",
    "                _ , maxlog, _ = yeojohnson(column_data)\n",
    "                if maxlog < 0:\n",
    "                    transformation_methods[col_name] = 'yeo_johnson'\n",
    "                    transformer = PowerTransformer(method='yeo-johnson')\n",
    "                    df_transformed[col_name] = transformer.fit_transform(column_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "                else:\n",
    "                    transformation_methods[col_name] = 'boxcox'\n",
    "                    transformer = PowerTransformer(method='box-cox')\n",
    "                    df_transformed[col_name] = transformer.fit_transform(column_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "            except ValueError:\n",
    "                transformation_methods[col_name] = 'square'\n",
    "                df_transformed[col_name] = np.square(column_data)\n",
    "                \n",
    "    return transformation_methods, df_transformed\n",
    "\n",
    "transformation_methods, df_transformed = select_feature_transformation_methods(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb5cbd-fbe9-4695-8a93-1877607ab5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a85b42-8665-41e0-bca8-35f877bceaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d601949-bcf4-44a9-a98b-6f227f2221d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_numerical(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8c99d-08a6-4077-9ff7-f3145d97c1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9f128-b903-4f19-b847-cd6454077908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
